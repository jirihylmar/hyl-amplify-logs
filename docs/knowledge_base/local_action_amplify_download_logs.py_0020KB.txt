=== File: local/action_amplify_download_logs.py ===
=== Size: 20KB ===

#!/usr/bin/env python3
"""
AWS Amplify Logs Downloader

This script downloads AWS Amplify access logs for multiple applications over specified date ranges.
It handles configuration loading, automatic chunking of date ranges, and uploading to S3.

Example usage:
    # Using command line arguments:
    python3 /home/hylmarj/hyl-amplify-logs/local/action_amplify_download_logs.py --config-path /home/hylmarj/hyl-amplify-logs/local/config.json --start-date 2023-10-01 --end-date 2023-10-31
    
    # Using a specific application:
    python3 /home/hylmarj/hyl-amplify-logs/local/action_amplify_download_logs.py --profile HylmarJ --region eu-west-1 --app-id d3hgg9jtwyuijn \
        --domain-name danse.tech --app-name danse_tech --start-date 2023-10-01 --end-date 2023-10-31
"""

import argparse
import json
import logging
import os
import subprocess
import time
from datetime import datetime, timedelta
from pathlib import Path
import requests
import boto3
from typing import Dict, List, Tuple, Optional, Union, Any
import sys

# Add the parent directory to the path to ensure imports work correctly
script_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(script_dir)
sys.path.append(parent_dir)

# Import the local config_loader
from local.config_loader import load_config

# Create logs directory if it doesn't exist
log_dir = os.path.join(script_dir, "logs")
os.makedirs(log_dir, exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join(log_dir, 'amplify_logs.log'))
    ]
)
logger = logging.getLogger(__name__)


class AmplifyLogDownloader:
    """
    Class for downloading AWS Amplify logs and uploading to S3
    """
    
    def __init__(self, config_path: Optional[str] = None, config_dict: Optional[Dict] = None):
        """
        Initialize the log downloader
        
        Args:
            config_path: Path to configuration JSON file
            config_dict: Configuration dictionary
        """
        self.config_loader = load_config(config_path, config_dict)
        self.applications = self.config_loader.get_applications()
        self.s3_config = self.config_loader.get_s3_config()
        self.chunk_size_days = self.config_loader.get_time_chunk_size()
        self.s3_client = None
        self.stats = {
            'total_chunks': 0,
            'successful_chunks': 0,
            'failed_chunks': 0,
            'upload_failures': 0,
            'failed_ranges': []
        }
    
    def get_amplify_logs(self, app: Dict[str, str], start_time: datetime, end_time: datetime) -> Union[str, Dict, None]:
        """
        Run AWS Amplify CLI command to get access logs for a specific time range
        
        Args:
            app: Application configuration dictionary
            start_time: Start time for log retrieval
            end_time: End time for log retrieval
            
        Returns:
            Log content as string, response dictionary, "REDUCE_RANGE" signal, or None on failure
        """
        command = [
            "aws", "amplify", "generate-access-logs",
            "--profile", app['profile'],
            "--region", app['region'],
            "--app-id", app['appId'],
            "--domain-name", app['domainName'],
            "--start-time", start_time.isoformat(timespec='seconds'),
            "--end-time", end_time.isoformat(timespec='seconds'),
            "--no-paginate"
        ]
        
        try:
            logger.info(f"Fetching logs for {app['appName']} from {start_time} to {end_time}")
            result = subprocess.run(command, capture_output=True, text=True, check=True)
            
            if result.stdout:
                response = json.loads(result.stdout)
                if 'logUrl' in response:
                    logger.info(f"Got log URL for {app['appName']} ({start_time} - {end_time})")
                    log_content = requests.get(response['logUrl'])
                    if log_content.status_code == 200:
                        return log_content.text
                    else:
                        logger.error(f"Failed to download logs from URL: HTTP {log_content.status_code}")
                        return None
                return response
            
            logger.warning(f"Empty response when fetching logs for {app['appName']}")
            return None
        
        except subprocess.CalledProcessError as e:
            if "reduce time range" in e.stderr:
                logger.warning(f"AWS API requested to reduce time range for {app['appName']}")
                return "REDUCE_RANGE"
            logger.error(f"Error running command for {app['appName']} ({start_time} - {end_time}): {e.stderr}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error for {app['appName']}: {str(e)}")
            return None
    
    def save_logs_locally(self, logs: Union[str, Dict], timestamp: datetime, app_name: str, output_dir: Optional[Path] = None) -> Path:
        """
        Save logs to local directory with specified path structure
        
        Args:
            logs: Log content (string or dictionary)
            timestamp: Timestamp for the logs
            app_name: Application name
            output_dir: Custom output directory (optional)
            
        Returns:
            Path to the saved log file
        """
        if output_dir is None:
            # Use default path structure
            base_path = Path(f"logs/type=amplify_logs/app={app_name}")
        else:
            # Use custom path structure
            base_path = output_dir / f"app={app_name}"
        
        date_path = base_path / f"date_export={timestamp.strftime('%Y-%m-%d')}"
        date_path.mkdir(parents=True, exist_ok=True)
        
        log_file = date_path / f"log_{timestamp.strftime('%Y%m%d_%H%M%S')}"
        
        with open(log_file, 'w') as f:
            if isinstance(logs, str):
                f.write(logs)
            else:
                json.dump(logs, f, indent=2)
        
        logger.info(f"Saved logs to {log_file}")
        return log_file
    
    def upload_to_s3(self, local_file: Path, app_name: str, timestamp: datetime, delete_after_upload: bool = False) -> bool:
        """
        Upload logs to S3 bucket with the correct path structure
        
        Args:
            local_file: Path to local log file
            app_name: Application name
            timestamp: Timestamp for the logs
            delete_after_upload: Whether to delete the local file after successful upload
            
        Returns:
            True if upload successful, False otherwise
        """
        if not self.s3_client:
            # Initialize S3 client
            self.s3_client = boto3.client('s3')
        
        bucket = self.s3_config['bucket']
        prefix = self.s3_config.get('prefix', '')
        
        # Construct S3 key
        s3_key = os.path.join(
            prefix,
            "type=amplify_logs",
            f"app={app_name}",
            f"date_export={timestamp.strftime('%Y-%m-%d')}",
            f"log_{timestamp.strftime('%Y%m%d_%H%M%S')}"
        )
        
        try:
            logger.info(f"Uploading {local_file} to s3://{bucket}/{s3_key}")
            self.s3_client.upload_file(str(local_file), bucket, s3_key)
            logger.info(f"Upload successful")
            
            # Delete local file if requested
            if delete_after_upload:
                try:
                    os.remove(local_file)
                    logger.info(f"Deleted local file {local_file} after successful upload")
                except Exception as e:
                    logger.warning(f"Failed to delete local file {local_file}: {str(e)}")
            
            return True
        except Exception as e:
            logger.error(f"Failed to upload to S3: {str(e)}")
            self.stats['upload_failures'] += 1
            return False
    
    def process_time_range(self, app: Dict[str, str], start_time: datetime, end_time: datetime, 
                          output_dir: Optional[Path] = None, depth: int = 0, delete_after_upload: bool = False) -> bool:
        """
        Process a time range with recursive retry using smaller chunks
        
        Args:
            app: Application configuration dictionary
            start_time: Start time for log retrieval
            end_time: End time for log retrieval
            output_dir: Custom output directory (optional)
            depth: Current recursion depth
            delete_after_upload: Whether to delete local files after successful S3 upload
            
        Returns:
            True if processing successful, False otherwise
        """
        if depth > 2:  # Limit recursion depth
            logger.warning(f"Max retry depth reached for {app['appName']} ({start_time} - {end_time})")
            return False
        
        hours_diff = (end_time - start_time).total_seconds() / 3600
        logger.info(f"Processing range (depth {depth}): {start_time} - {end_time} ({hours_diff:.1f} hours)")
        
        logs = self.get_amplify_logs(app, start_time, end_time)
        
        if logs == "REDUCE_RANGE":
            logger.info(f"Splitting time range into smaller chunks for {app['appName']}")
            
            # Split the range into two parts
            mid_time = start_time + (end_time - start_time) // 2
            
            # Process both halves
            success1 = self.process_time_range(app, start_time, mid_time, output_dir, depth + 1, delete_after_upload)
            time.sleep(1)  # Add delay between requests
            success2 = self.process_time_range(app, mid_time, end_time, output_dir, depth + 1, delete_after_upload)
            
            return success1 or success2
        
        elif logs:
            local_file = self.save_logs_locally(logs, end_time, app['appName'], output_dir)
            
            # Only attempt S3 upload if bucket is configured
            if 'bucket' in self.s3_config and self.s3_config['bucket']:
                return self.upload_to_s3(local_file, app['appName'], end_time, delete_after_upload)
            return True
        
        return False
    
    def generate_time_ranges(self, start_date: datetime.date, end_date: datetime.date) -> List[Tuple[datetime, datetime]]:
        """
        Generate time ranges based on the configured chunk size
        
        Args:
            start_date: Start date for log retrieval
            end_date: End date for log retrieval
            
        Returns:
            List of (start_time, end_time) tuples
        """
        # Convert dates to datetime objects with time at midnight
        start_time = datetime.combine(start_date, datetime.min.time())
        # Use time with no microseconds for consistent testing
        end_time = datetime.combine(end_date, datetime.max.time().replace(microsecond=0))
        
        ranges = []
        current_start = start_time
        
        while current_start <= end_time:
            # Each chunk is exactly {chunk_size_days} days (or less for the final chunk)
            chunk_end = min(
                current_start + timedelta(days=self.chunk_size_days) - timedelta(seconds=1),
                end_time
            )
            ranges.append((current_start, chunk_end))
            current_start = chunk_end + timedelta(seconds=1)
        
        return ranges
    
    def process_application(self, app: Dict[str, str], start_date: datetime.date, 
                           end_date: datetime.date, output_dir: Optional[Path] = None,
                           delete_after_upload: bool = False) -> Dict[str, Any]:
        """
        Process a single application for the given date range
        
        Args:
            app: Application configuration dictionary
            start_date: Start date for log retrieval
            end_date: End date for log retrieval
            output_dir: Custom output directory (optional)
            delete_after_upload: Whether to delete local files after successful S3 upload
            
        Returns:
            Dictionary with statistics for this application
        """
        app_stats = {
            'app_name': app['appName'],
            'total_chunks': 0,
            'successful_chunks': 0,
            'failed_chunks': 0,
            'failed_ranges': []
        }
        
        time_ranges = self.generate_time_ranges(start_date, end_date)
        app_stats['total_chunks'] = len(time_ranges)
        self.stats['total_chunks'] += app_stats['total_chunks']
        
        logger.info(f"Processing {app['appName']}: {len(time_ranges)} chunks from {start_date} to {end_date}")
        
        for i, (start_time, end_time) in enumerate(time_ranges, 1):
            logger.info(f"Processing chunk {i}/{len(time_ranges)} for {app['appName']}")
            
            if self.process_time_range(app, start_time, end_time, output_dir, delete_after_upload=delete_after_upload):
                app_stats['successful_chunks'] += 1
                self.stats['successful_chunks'] += 1
            else:
                app_stats['failed_chunks'] += 1
                self.stats['failed_chunks'] += 1
                app_stats['failed_ranges'].append((start_time, end_time))
                self.stats['failed_ranges'].append({
                    'app_name': app['appName'],
                    'start_time': start_time.isoformat(),
                    'end_time': end_time.isoformat()
                })
            
            # Add delay between chunks
            time.sleep(1)
        
        return app_stats
    
    def process_all_applications(self, start_date: datetime.date, end_date: datetime.date, 
                                output_dir: Optional[Path] = None, delete_after_upload: bool = False) -> Dict:
        """
        Process all configured applications for the given date range
        
        Args:
            start_date: Start date for log retrieval
            end_date: End date for log retrieval
            output_dir: Custom output directory (optional)
            delete_after_upload: Whether to delete local files after successful S3 upload
            
        Returns:
            Dictionary with overall statistics
        """
        logger.info(f"Starting download for {len(self.applications)} applications from {start_date} to {end_date}")
        logger.info(f"Applications to process: {[app['appName'] for app in self.applications]}")
        
        app_results = []
        
        for i, app in enumerate(self.applications):
            logger.info(f"Processing application {i+1}/{len(self.applications)}: {app['appName']} ({app['appId']})")
            app_stats = self.process_application(
                app, start_date, end_date, output_dir, delete_after_upload
            )
            app_results.append(app_stats)
            logger.info(f"Completed processing application: {app['appName']}")
        
        logger.info(f"All {len(self.applications)} applications processed")
        return {
            'overall_stats': self.stats,
            'app_results': app_results
        }


def parse_date(date_str: str) -> datetime.date:
    """Parse date string in YYYY-MM-DD format"""
    try:
        return datetime.strptime(date_str, '%Y-%m-%d').date()
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid date format: {date_str}. Use YYYY-MM-DD")


def main():
    parser = argparse.ArgumentParser(description='Download AWS Amplify logs for a specific time range')
    
    # Configuration options
    config_group = parser.add_argument_group('Configuration')
    config_group.add_argument('--config-path', type=str, help='Path to configuration JSON file')
    
    # Date range options
    date_group = parser.add_argument_group('Date Range')
    date_group.add_argument('--start-date', type=parse_date, required=True, help='Start date (YYYY-MM-DD)')
    date_group.add_argument('--end-date', type=parse_date, required=True, help='End date (YYYY-MM-DD)')
    
    # Output options
    output_group = parser.add_argument_group('Output')
    output_group.add_argument('--output-dir', type=Path, 
                             default=Path('/home/hylmarj/_scratch/type=amplify_logs/'),
                             help='Base path for saving logs (default: /home/hylmarj/_scratch/type=amplify_logs/)')
    output_group.add_argument('--delete-after-upload', action='store_true',
                             help='Delete local log files after successful S3 upload')
    
    # Single app options (alternative to config file)
    app_group = parser.add_argument_group('Single Application (alternative to config file)')
    app_group.add_argument('--profile', type=str, help='AWS profile name')
    app_group.add_argument('--region', type=str, help='AWS region')
    app_group.add_argument('--app-id', type=str, help='Amplify app ID')
    app_group.add_argument('--domain-name', type=str, help='Domain name')
    app_group.add_argument('--app-name', type=str, help='Application name')
    app_group.add_argument('--s3-bucket', type=str, help='S3 bucket for logs')
    app_group.add_argument('--s3-prefix', type=str, default='', help='S3 prefix for logs')
    
    args = parser.parse_args()
    
    # Create output directory if it doesn't exist
    output_dir = args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"Output directory created/verified: {output_dir}")
    
    # Determine if using config file or single app parameters
    config_dict = None
    if args.config_path:
        # Use configuration file
        downloader = AmplifyLogDownloader(config_path=args.config_path)
    elif all([args.profile, args.region, args.app_id, args.domain_name, args.app_name]):
        # Create configuration from command line arguments
        config_dict = {
            'applications': [
                {
                    'profile': args.profile,
                    'region': args.region,
                    'appId': args.app_id,
                    'domainName': args.domain_name,
                    'appName': args.app_name
                }
            ],
            's3': {
                'bucket': args.s3_bucket or '',
                'prefix': args.s3_prefix or ''
            },
            'timeChunkSize': {
                'days': 14
            }
        }
        downloader = AmplifyLogDownloader(config_dict=config_dict)
    else:
        parser.error("Either --config-path or all single application parameters must be provided")
        return
    
    # Process all applications
    results = downloader.process_all_applications(
        args.start_date, args.end_date, args.output_dir, args.delete_after_upload
    )
    
    # Save results to file
    results_file = output_dir / "amplify_logs_results.json"
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2, default=str)
    
    # Print summary
    print("\nDownload Summary:")
    print(f"Applications processed: {len(results['app_results'])}")
    for app_result in results['app_results']:
        print(f"  - {app_result['app_name']}: {app_result['successful_chunks']} successful, {app_result['failed_chunks']} failed")
    print(f"Total chunks attempted: {results['overall_stats']['total_chunks']}")
    print(f"Successful downloads: {results['overall_stats']['successful_chunks']}")
    print(f"Failed chunks: {results['overall_stats']['failed_chunks']}")
    print(f"Upload failures: {results['overall_stats']['upload_failures']}")
    print(f"Delete after upload: {'Enabled' if args.delete_after_upload else 'Disabled'}")
    
    if results['overall_stats']['failed_ranges']:
        print("\nFailed time ranges:")
        for failed_range in results['overall_stats']['failed_ranges']:
            print(f"- {failed_range['app_name']}: {failed_range['start_time']} to {failed_range['end_time']}")
    
    print(f"\nDetailed results saved to {results_file}")


if __name__ == "__main__":
    main()